https://medium.com/containermind/a-beginners-guide-to-kubernetes-7e8ca56420b6


Master: t2.medium
Worker1: t2.micro (free)
Worker2: t2.micro (free)

1. Save the .pem file while creating the aws machine.
2. use puttygen and load the pem file and save as private key .ppk file
3. Then, get the public ip and connect using putty. pass the ppk file in SSh->AUTH section (username : centos) connect to master and worker nodes
4. run the below mentioned commands to install docker engine and kubernetes
Install docker from this url:
https://docs.docker.com/install/linux/docker-ce/centos/

Install from this website:
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl

5. Run: hostnamectl set-hostname --static k8smaster in master
6. hostnamectl set-hostname --static k8snode1 (run in worker1)
7. hostnamectl set-hostname --static k8snode2 (run in worker2)
8. Run "vi /etc/cloud/cloud.cfg" and add "preserve_hostname: true" at the end (in master)
9. Run "vi /etc/cloud/cloud.cfg" and add "preserve_hostname: true" at the end (in worker 1 & 2)
10. Edit /ect/hosts and add private ips of manager and workers in all the machines ath the end of the file
e.g:
172.31.3.2 k8smaster
172.31.35.229 k8snode2
172.31.40.66 k8snode1
11. Run "reboot" command in all 3 machines
12. Run "kubeadm init" in master machine
13. We will see below msg:
[Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.31.3.2:6443 --token skwpel.mylzyxmv45p649j2 \
    --discovery-token-ca-cert-hash sha256:fde49b1c3291330b7c8971db5714e762dda4bdad2b83c90fb2cdf05b5ec74b17
]
14. Go to master node and open 'security groups' and add an entry for port 6443
15. Run the given commands in master and commands in workers
16. Run "kubectl get nodes" in master to chk the newly added worker nodes
17. Run: "kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"" to install network plugin
so that master and workers can communicate with each other.
[The Weave Net addon for Kubernetes comes with a Network Policy Controller]
18. Run "kubectl get nodes" in master to chk the newly status of added worker nodes. It shoud be 'ready'
19. Run 'kubectl get pods --all-namespaces' to get details of kubernetes components
20. chk namespaces: "kubectl get namespaces"
21. Create Namespace:
 vi dev.yml
   apiVersion: v1
   kind: Namespace
   metadata:
       name: development
create namespace: "kubectl create -f dev.yml"
verify namespace: "kubectl get ns  "
22. Create Pod:
  apiVersion: v1
  kind: Pod
  metadata:
    name: nginx-deployment
    labels:
      name: nginx
  spec:
    containers:
    - name: nginx
      image: nginx: latest
      ports:
      - containerPort: 80
create Pod: "kubectl apply -f pod.yml"
verify pod: "kubectl get pods"
23. we need to open some port to allow communication to k8s cluster in aws portal
24. Run "kubectl describe pod nginx-deployment"   [nginx-depl.. is the pod name]
25. Create namespace with a command instead of yaml file: "kubectl create namespace quota-mem-cpu-example"
26. create a yaml for allocating resourcequota:
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mem-cpu-demo
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
27. Apply the quota:
"kubectl apply -f limits.yml --namespace=quota-mem-cpu-example"
28. Check if really the quota has been allocated to namespace (output in yaml format):
"kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml"
29. Apply Pod to a new namespace:
apiVersion: v1
kind: Pod
metadata:
  name: quota-mem-cpu-demo
spec:
  containers:
  - name: quota-mem-cpu-demo-cotr
    image: nginx
    resources:
      limits:
        memory: "800Mi"
        cpu: "800m"
      requests:
        memory: "600Mi"
        cpu: "400m"
30. Run: "kubectl apply -f deploy.yml --namespace=quota-mem-cpu-example"
31. chk used quota by running:
"kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml"
32. deploy2.yml
apiVersion: v1
kind: Pod
metadata:
  name: quota-mem-cpu-demo-2
spec:
  containers:
  - name: quota-mem-cpu-demo-cotr2
    image: nginx
    resources:
      limits:
        memory: "1Gi"
        cpu: "800m"
      requests:
        memory: "700Mi"
        cpu: "400m"
        
33. kubectl apply -f deploy2.yml --namespace=quota-mem-cpu-example
Output:
Error from server (Forbidden): error when creating "deploy2.yml": pods "quota-mem-cpu-demo-2" is forbidden: exceeded quota: mem-cpu-demo, requested: requests.memory=700Mi, used: requests.memory=600Mi, limited: requests.memory=1Gi       

We have eror because we have limited memomy in namespace to 1gb, and pod1 in deploy.yml has taken 400mb, and pod2 in deploy2.yml is requesting for 700mb.

34. 


==========================================================================================
[root@ip-172-31-8-5 ~]# history
    1  yum install -y yum-utils   device-mapper-persistent-data   lvm2
    2   yum-config-manager     --add-repo     https://download.docker.com/linux/centos/docker-ce.repo
    3  yum install docker-ce docker-ce-cli containerd.io
    4  service docker start
    5  chkconfig docker on
    6  cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

    7  setenforce 0
    8  sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
    9  yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
   10  systemctl enable --now kubelet
   11  cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

   12  sysctl --system
   13  systemctl daemon-reload
   14  systemctl restart kubelet
   15  history
